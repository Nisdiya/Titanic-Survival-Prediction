# Titanic-Survival-Prediction
Machine Learning project on Titanic dataset (Kaggle). Includes EDA, preprocessing, Logistic Regression, Decision Tree, Random Forest, and XGBoost models with evaluation and submission file.
# Titanic Survival Prediction 🚢

This project is based on the famous Titanic dataset from Kaggle.  
Goal: Predict survival of passengers using ML models.
This was my first ever ML project where i learned about lots of new terms, overfitting, underfitting, bagging, boosting, min_samples_split and lot more.

## 🔹 Steps Covered
- Data Cleaning & Preprocessing
- Exploratory Data Analysis (EDA)
- Feature Engineering
- Model Building:
  - Logistic Regression
  - Decision Tree Classifier
  - Random Forest (tuned)
  - XGBoost
- Model Evaluation
- Kaggle Submission (Score: 0.76315)

## 📊 Results
- Best Model: Random Forest (manual tuning)
- Test Accuracy: ~81%
- Kaggle Public Score: 0.76315

## 🛠️ Tools Used
- Python, Pandas, NumPy
- Matplotlib, Seaborn
- Scikit-learn
- XGBoost
## 📖 Learnings
- Understood the concepts of overfitting and underfitting.
- Learned how bagging (Random Forest) and boosting (XGBoost) improve performance.
- Explored hyperparameter tuning (max_depth, min_samples_split, etc.).
- Gained experience in submitting predictions on Kaggle.
- Understood which model to be used with complex datasets and which one to use with simpler ones.
- Learned error solving by own
